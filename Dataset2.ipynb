{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel(\"createsdataset.xlsx\")\n",
    "data.to_csv('createsdataset.csv',index=True)\n",
    "data=data.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of      Temperature©[min]  D.O. (mg/L)[max]  pH[min]  \\\n",
      "0                 17.0               8.6     8.40   \n",
      "1                 30.0               7.8     8.30   \n",
      "2                 33.0               6.0     8.20   \n",
      "3                 34.0               7.0     8.60   \n",
      "4                 34.0               7.3     8.20   \n",
      "..                 ...               ...      ...   \n",
      "134               20.0               7.1     7.30   \n",
      "135               24.0               7.4     6.90   \n",
      "136               19.0               8.1     6.10   \n",
      "137               21.0               7.5     7.20   \n",
      "138               16.0               8.2     6.78   \n",
      "\n",
      "     Conductivity(μSiemens/cm)[min]  BOD (mg/L)[min]  Total coliform[min]  \n",
      "0                             542.0             2.70               9000.0  \n",
      "1                             492.0             2.80               4600.0  \n",
      "2                             750.0             2.90               5000.0  \n",
      "3                             562.0             2.40              16000.0  \n",
      "4                             622.0             2.30              29000.0  \n",
      "..                              ...              ...                  ...  \n",
      "134                           217.0             1.00              11000.0  \n",
      "135                           253.0             1.40              11000.0  \n",
      "136                           302.0             1.30              13000.0  \n",
      "137                           301.0             1.40              11000.0  \n",
      "138                           231.0             1.05              13000.0  \n",
      "\n",
      "[124 rows x 6 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data,test_size=0.1,random_state=100)\n",
    "train_features = train.loc[:,data.columns !='D.O. (mg/L)[max]']\n",
    "test_features = test.loc[:,data.columns !='D.O. (mg/L)[max]']\n",
    "train_label = train['D.O. (mg/L)[max]']\n",
    "test_label = test['D.O. (mg/L)[max]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111, 5)\n",
      "(111,)\n",
      "(13, 5)\n",
      "(13,)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape)\n",
    "print(train_label.shape)\n",
    "print(test_features.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = preprocessing.MinMaxScaler(feature_range=(0, 1)).fit_transform(train_features)\n",
    "test_features = preprocessing.MinMaxScaler(feature_range=(0, 1)).fit_transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANFIS MODEL CODE\n",
    "from Models import myanfis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9223372036854775807"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.maxsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = myanfis.fis_parameters(\n",
    "    n_input=5,                # no. of Regressors\n",
    "    n_memb=10,                 # no. of fuzzy memberships\n",
    "    batch_size=1,            # 16 / 32 / 64 / ...\n",
    "    memb_func='gaussian',      # 'gaussian' / 'gbellmf'\n",
    "    optimizer='sgd',          # sgd / adam / ...\n",
    "    # mse / mae / huber_loss / mean_absolute_percentage_error / ...\n",
    "    loss='mse',\n",
    "    n_epochs=150               # 10 / 25 / 50 / 100 / ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "fis = myanfis.ANFIS(n_input=param.n_input,\n",
    "                    n_memb=param.n_memb,\n",
    "                    batch_size=param.batch_size,\n",
    "                    memb_func=param.memb_func,\n",
    "                    name='myanfis'\n",
    "                    )\n",
    "\n",
    "# fis = myanfis.ANFIS(n_input=6,\n",
    "#                     n_memb=3,\n",
    "#                     batch_size=1,\n",
    "#                     memb_func='gaussian',\n",
    "#                     name='myanfis'\n",
    "#                     )\n",
    "# compile model\n",
    "fis.model.compile(optimizer=param.optimizer,\n",
    "                    loss=param.loss,\n",
    "                    metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "111/111 [==============================] - 2s 8ms/step - loss: 7593.8633 - mse: 7593.8633 - val_loss: 728.5397 - val_mse: 728.5397\n",
      "Epoch 2/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7585.8784 - mse: 7585.8784 - val_loss: 711.5109 - val_mse: 711.5109\n",
      "Epoch 3/150\n",
      "111/111 [==============================] - 1s 5ms/step - loss: 7552.5581 - mse: 7552.5581 - val_loss: 665.2390 - val_mse: 665.2390\n",
      "Epoch 4/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7444.8481 - mse: 7444.8481 - val_loss: 622.9696 - val_mse: 622.9696\n",
      "Epoch 5/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7441.8687 - mse: 7441.8687 - val_loss: 610.2928 - val_mse: 610.2928\n",
      "Epoch 6/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7423.7788 - mse: 7423.7788 - val_loss: 605.8282 - val_mse: 605.8282\n",
      "Epoch 7/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7425.8530 - mse: 7425.8530 - val_loss: 592.8421 - val_mse: 592.8421\n",
      "Epoch 8/150\n",
      "111/111 [==============================] - 1s 5ms/step - loss: 7425.3481 - mse: 7425.3481 - val_loss: 576.6310 - val_mse: 576.6310\n",
      "Epoch 9/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7417.5146 - mse: 7417.5146 - val_loss: 588.7448 - val_mse: 588.7448\n",
      "Epoch 10/150\n",
      "111/111 [==============================] - 1s 5ms/step - loss: 7438.3154 - mse: 7438.3154 - val_loss: 560.2776 - val_mse: 560.2776\n",
      "Epoch 11/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7420.0674 - mse: 7420.0674 - val_loss: 577.4703 - val_mse: 577.4703\n",
      "Epoch 12/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7440.4937 - mse: 7440.4937 - val_loss: 548.4123 - val_mse: 548.4123\n",
      "Epoch 13/150\n",
      "111/111 [==============================] - 1s 8ms/step - loss: 7383.4131 - mse: 7383.4131 - val_loss: 538.3121 - val_mse: 538.3121\n",
      "Epoch 14/150\n",
      "111/111 [==============================] - 1s 7ms/step - loss: 7364.6875 - mse: 7364.6875 - val_loss: 538.4139 - val_mse: 538.4139\n",
      "Epoch 15/150\n",
      "111/111 [==============================] - 1s 5ms/step - loss: 7360.8716 - mse: 7360.8716 - val_loss: 523.8536 - val_mse: 523.8536\n",
      "Epoch 16/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7350.2417 - mse: 7350.2417 - val_loss: 516.7707 - val_mse: 516.7707\n",
      "Epoch 17/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7345.4883 - mse: 7345.4883 - val_loss: 514.7692 - val_mse: 514.7692\n",
      "Epoch 18/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7342.6426 - mse: 7342.6426 - val_loss: 505.8898 - val_mse: 505.8898\n",
      "Epoch 19/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7335.5708 - mse: 7335.5708 - val_loss: 500.2719 - val_mse: 500.2719\n",
      "Epoch 20/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7321.0840 - mse: 7321.0840 - val_loss: 505.1764 - val_mse: 505.1764\n",
      "Epoch 21/150\n",
      "111/111 [==============================] - 1s 5ms/step - loss: 7321.3970 - mse: 7321.3970 - val_loss: 506.3153 - val_mse: 506.3153\n",
      "Epoch 22/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7322.7466 - mse: 7322.7466 - val_loss: 489.9453 - val_mse: 489.9453\n",
      "Epoch 23/150\n",
      "111/111 [==============================] - 1s 5ms/step - loss: 7406.3008 - mse: 7406.3008 - val_loss: 486.6914 - val_mse: 486.6914\n",
      "Epoch 24/150\n",
      "111/111 [==============================] - 1s 5ms/step - loss: 7307.8633 - mse: 7307.8633 - val_loss: 486.5930 - val_mse: 486.5930\n",
      "Epoch 25/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7307.0835 - mse: 7307.0835 - val_loss: 484.6698 - val_mse: 484.6698\n",
      "Epoch 26/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7303.1743 - mse: 7303.1743 - val_loss: 483.7732 - val_mse: 483.7732\n",
      "Epoch 27/150\n",
      "111/111 [==============================] - 1s 5ms/step - loss: 7299.1138 - mse: 7299.1138 - val_loss: 484.5344 - val_mse: 484.5344\n",
      "Epoch 28/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7287.4331 - mse: 7287.4331 - val_loss: 486.9605 - val_mse: 486.9605\n",
      "Epoch 29/150\n",
      "111/111 [==============================] - 1s 5ms/step - loss: 7289.9624 - mse: 7289.9624 - val_loss: 491.8576 - val_mse: 491.8576\n",
      "Epoch 30/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7286.9229 - mse: 7286.9229 - val_loss: 497.1200 - val_mse: 497.1200\n",
      "Epoch 31/150\n",
      "111/111 [==============================] - 1s 5ms/step - loss: 7361.4536 - mse: 7361.4536 - val_loss: 487.7852 - val_mse: 487.7852\n",
      "Epoch 32/150\n",
      "111/111 [==============================] - 1s 5ms/step - loss: 7289.3936 - mse: 7289.3936 - val_loss: 491.4902 - val_mse: 491.4902\n",
      "Epoch 33/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7261.5337 - mse: 7261.5337 - val_loss: 495.2152 - val_mse: 495.2152\n",
      "Epoch 34/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7283.0840 - mse: 7283.0840 - val_loss: 495.5187 - val_mse: 495.5187\n",
      "Epoch 35/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7275.8838 - mse: 7275.8838 - val_loss: 501.8520 - val_mse: 501.8520\n",
      "Epoch 36/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7276.3599 - mse: 7276.3599 - val_loss: 498.8535 - val_mse: 498.8535\n",
      "Epoch 37/150\n",
      "111/111 [==============================] - 1s 5ms/step - loss: 7274.5244 - mse: 7274.5244 - val_loss: 502.4215 - val_mse: 502.4215\n",
      "Epoch 38/150\n",
      "111/111 [==============================] - 1s 5ms/step - loss: 7372.9287 - mse: 7372.9287 - val_loss: 503.6982 - val_mse: 503.6982\n",
      "Epoch 39/150\n",
      "111/111 [==============================] - 1s 5ms/step - loss: 7271.2588 - mse: 7271.2588 - val_loss: 507.2029 - val_mse: 507.2029\n",
      "Epoch 40/150\n",
      "111/111 [==============================] - 1s 5ms/step - loss: 7262.1616 - mse: 7262.1616 - val_loss: 514.3937 - val_mse: 514.3937\n",
      "Epoch 41/150\n",
      "111/111 [==============================] - 1s 5ms/step - loss: 7265.0962 - mse: 7265.0962 - val_loss: 520.5908 - val_mse: 520.5908\n",
      "Epoch 42/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7266.5908 - mse: 7266.5908 - val_loss: 526.1371 - val_mse: 526.1371\n",
      "Epoch 43/150\n",
      "111/111 [==============================] - 1s 5ms/step - loss: 7265.0669 - mse: 7265.0669 - val_loss: 529.8868 - val_mse: 529.8868\n",
      "Epoch 44/150\n",
      "111/111 [==============================] - 1s 5ms/step - loss: 7257.5820 - mse: 7257.5820 - val_loss: 543.8766 - val_mse: 543.8766\n",
      "Epoch 45/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7258.8662 - mse: 7258.8662 - val_loss: 535.7885 - val_mse: 535.7885\n",
      "Epoch 46/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7247.9072 - mse: 7247.9072 - val_loss: 555.9792 - val_mse: 555.9792\n",
      "Epoch 47/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7256.8140 - mse: 7256.8140 - val_loss: 546.3812 - val_mse: 546.3812\n",
      "Epoch 48/150\n",
      "111/111 [==============================] - 1s 5ms/step - loss: 7247.5840 - mse: 7247.5840 - val_loss: 553.3328 - val_mse: 553.3328\n",
      "Epoch 49/150\n",
      "111/111 [==============================] - 1s 5ms/step - loss: 7256.5103 - mse: 7256.5103 - val_loss: 556.9525 - val_mse: 556.9525\n",
      "Epoch 50/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7253.6865 - mse: 7253.6865 - val_loss: 565.5812 - val_mse: 565.5812\n",
      "Epoch 51/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7254.5562 - mse: 7254.5562 - val_loss: 571.6456 - val_mse: 571.6456\n",
      "Epoch 52/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7251.1279 - mse: 7251.1279 - val_loss: 569.1412 - val_mse: 569.1412\n",
      "Epoch 53/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7414.4790 - mse: 7414.4790 - val_loss: 576.1282 - val_mse: 576.1282\n",
      "Epoch 54/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7252.0229 - mse: 7252.0229 - val_loss: 568.9102 - val_mse: 568.9102\n",
      "Epoch 55/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7252.5063 - mse: 7252.5063 - val_loss: 575.0332 - val_mse: 575.0332\n",
      "Epoch 56/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7251.5103 - mse: 7251.5103 - val_loss: 578.3101 - val_mse: 578.3101\n",
      "Epoch 57/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7247.8823 - mse: 7247.8823 - val_loss: 580.6732 - val_mse: 580.6732\n",
      "Epoch 58/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7240.3721 - mse: 7240.3721 - val_loss: 599.9547 - val_mse: 599.9547\n",
      "Epoch 59/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7246.7173 - mse: 7246.7173 - val_loss: 586.9781 - val_mse: 586.9781\n",
      "Epoch 60/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7248.7139 - mse: 7248.7139 - val_loss: 591.4476 - val_mse: 591.4476\n",
      "Epoch 61/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7248.3359 - mse: 7248.3359 - val_loss: 595.8555 - val_mse: 595.8555\n",
      "Epoch 62/150\n",
      "111/111 [==============================] - 1s 7ms/step - loss: 7240.3228 - mse: 7240.3228 - val_loss: 606.9858 - val_mse: 606.9858\n",
      "Epoch 63/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7247.4897 - mse: 7247.4897 - val_loss: 605.8533 - val_mse: 605.8533\n",
      "Epoch 64/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7246.2109 - mse: 7246.2109 - val_loss: 606.2789 - val_mse: 606.2789\n",
      "Epoch 65/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7237.4839 - mse: 7237.4839 - val_loss: 596.6151 - val_mse: 596.6151\n",
      "Epoch 66/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7243.4526 - mse: 7243.4526 - val_loss: 607.3558 - val_mse: 607.3558\n",
      "Epoch 67/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7230.2646 - mse: 7230.2646 - val_loss: 501.0323 - val_mse: 501.0323\n",
      "Epoch 68/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7266.8740 - mse: 7266.8740 - val_loss: 504.7205 - val_mse: 504.7205\n",
      "Epoch 69/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7258.1396 - mse: 7258.1396 - val_loss: 506.9186 - val_mse: 506.9186\n",
      "Epoch 70/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7261.2720 - mse: 7261.2720 - val_loss: 511.1530 - val_mse: 511.1530\n",
      "Epoch 71/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7250.3242 - mse: 7250.3242 - val_loss: 518.8178 - val_mse: 518.8178\n",
      "Epoch 72/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7255.9414 - mse: 7255.9414 - val_loss: 524.7704 - val_mse: 524.7704\n",
      "Epoch 73/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7250.0889 - mse: 7250.0889 - val_loss: 655.0201 - val_mse: 655.0201\n",
      "Epoch 74/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7239.4712 - mse: 7239.4712 - val_loss: 661.5187 - val_mse: 661.5187\n",
      "Epoch 75/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7241.2119 - mse: 7241.2119 - val_loss: 639.4437 - val_mse: 639.4437\n",
      "Epoch 76/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7232.6094 - mse: 7232.6094 - val_loss: 678.5275 - val_mse: 678.5275\n",
      "Epoch 77/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7243.7920 - mse: 7243.7920 - val_loss: 634.1158 - val_mse: 634.1158\n",
      "Epoch 78/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7238.2173 - mse: 7238.2173 - val_loss: 637.3391 - val_mse: 637.3391\n",
      "Epoch 79/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7229.5801 - mse: 7229.5801 - val_loss: 671.3955 - val_mse: 671.3955\n",
      "Epoch 80/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7241.9238 - mse: 7241.9238 - val_loss: 659.5675 - val_mse: 659.5675\n",
      "Epoch 81/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7237.5576 - mse: 7237.5576 - val_loss: 657.6231 - val_mse: 657.6231\n",
      "Epoch 82/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7240.2324 - mse: 7240.2324 - val_loss: 644.4645 - val_mse: 644.4645\n",
      "Epoch 83/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7235.9019 - mse: 7235.9019 - val_loss: 650.4003 - val_mse: 650.4003\n",
      "Epoch 84/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7233.9199 - mse: 7233.9199 - val_loss: 615.2125 - val_mse: 615.2125\n",
      "Epoch 85/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7234.3013 - mse: 7234.3013 - val_loss: 630.1187 - val_mse: 630.1187\n",
      "Epoch 86/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7235.2368 - mse: 7235.2368 - val_loss: 640.3801 - val_mse: 640.3801\n",
      "Epoch 87/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7233.2432 - mse: 7233.2432 - val_loss: 631.0322 - val_mse: 631.0322\n",
      "Epoch 88/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7228.9443 - mse: 7228.9443 - val_loss: 666.1875 - val_mse: 666.1875\n",
      "Epoch 89/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7232.1665 - mse: 7232.1665 - val_loss: 628.9070 - val_mse: 628.9070\n",
      "Epoch 90/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7230.9116 - mse: 7230.9116 - val_loss: 661.2363 - val_mse: 661.2363\n",
      "Epoch 91/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7225.5952 - mse: 7225.5952 - val_loss: 588.1942 - val_mse: 588.1942\n",
      "Epoch 92/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7225.6660 - mse: 7225.6660 - val_loss: 585.7940 - val_mse: 585.7940\n",
      "Epoch 93/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7231.6470 - mse: 7231.6470 - val_loss: 662.6884 - val_mse: 662.6884\n",
      "Epoch 94/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7236.1787 - mse: 7236.1787 - val_loss: 658.6464 - val_mse: 658.6464\n",
      "Epoch 95/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7226.6997 - mse: 7226.6997 - val_loss: 635.3812 - val_mse: 635.3812\n",
      "Epoch 96/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7236.5815 - mse: 7236.5815 - val_loss: 650.4767 - val_mse: 650.4767\n",
      "Epoch 97/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7229.1172 - mse: 7229.1172 - val_loss: 644.0704 - val_mse: 644.0704\n",
      "Epoch 98/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7237.0410 - mse: 7237.0410 - val_loss: 658.7285 - val_mse: 658.7285\n",
      "Epoch 99/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7220.3569 - mse: 7220.3569 - val_loss: 628.9830 - val_mse: 628.9830\n",
      "Epoch 100/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7217.0078 - mse: 7217.0078 - val_loss: 677.1046 - val_mse: 677.1046\n",
      "Epoch 101/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7233.2217 - mse: 7233.2217 - val_loss: 676.9773 - val_mse: 676.9773\n",
      "Epoch 102/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7234.3682 - mse: 7234.3682 - val_loss: 669.7989 - val_mse: 669.7989\n",
      "Epoch 103/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7229.9111 - mse: 7229.9111 - val_loss: 657.3123 - val_mse: 657.3123\n",
      "Epoch 104/150\n",
      "111/111 [==============================] - 1s 7ms/step - loss: 7227.3301 - mse: 7227.3301 - val_loss: 677.0858 - val_mse: 677.0858\n",
      "Epoch 105/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7231.6089 - mse: 7231.6089 - val_loss: 680.3929 - val_mse: 680.3929\n",
      "Epoch 106/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7234.4536 - mse: 7234.4536 - val_loss: 676.2872 - val_mse: 676.2872\n",
      "Epoch 107/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7228.6646 - mse: 7228.6646 - val_loss: 655.1296 - val_mse: 655.1296\n",
      "Epoch 108/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7233.5029 - mse: 7233.5029 - val_loss: 664.1257 - val_mse: 664.1257\n",
      "Epoch 109/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7231.0747 - mse: 7231.0747 - val_loss: 670.9417 - val_mse: 670.9417\n",
      "Epoch 110/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7233.4380 - mse: 7233.4380 - val_loss: 663.3919 - val_mse: 663.3919\n",
      "Epoch 111/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7227.4668 - mse: 7227.4668 - val_loss: 655.8594 - val_mse: 655.8594\n",
      "Epoch 112/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7229.8887 - mse: 7229.8887 - val_loss: 669.4615 - val_mse: 669.4615\n",
      "Epoch 113/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7224.3530 - mse: 7224.3530 - val_loss: 656.2804 - val_mse: 656.2804\n",
      "Epoch 114/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7229.8350 - mse: 7229.8350 - val_loss: 668.5010 - val_mse: 668.5010\n",
      "Epoch 115/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7227.1621 - mse: 7227.1621 - val_loss: 682.1843 - val_mse: 682.1843\n",
      "Epoch 116/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7233.5596 - mse: 7233.5596 - val_loss: 670.8349 - val_mse: 670.8349\n",
      "Epoch 117/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7226.6240 - mse: 7226.6240 - val_loss: 680.7593 - val_mse: 680.7593\n",
      "Epoch 118/150\n",
      "111/111 [==============================] - 1s 7ms/step - loss: 7228.9868 - mse: 7228.9868 - val_loss: 664.2148 - val_mse: 664.2148\n",
      "Epoch 119/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7225.3472 - mse: 7225.3472 - val_loss: 660.9854 - val_mse: 660.9854\n",
      "Epoch 120/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7232.6089 - mse: 7232.6089 - val_loss: 668.1304 - val_mse: 668.1304\n",
      "Epoch 121/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7225.8677 - mse: 7225.8677 - val_loss: 665.5093 - val_mse: 665.5093\n",
      "Epoch 122/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7233.3232 - mse: 7233.3232 - val_loss: 669.1790 - val_mse: 669.1790\n",
      "Epoch 123/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7230.8447 - mse: 7230.8447 - val_loss: 670.5424 - val_mse: 670.5424\n",
      "Epoch 124/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7231.4883 - mse: 7231.4883 - val_loss: 672.6006 - val_mse: 672.6006\n",
      "Epoch 125/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7229.2534 - mse: 7229.2534 - val_loss: 672.4359 - val_mse: 672.4359\n",
      "Epoch 126/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7228.5889 - mse: 7228.5889 - val_loss: 682.8844 - val_mse: 682.8844\n",
      "Epoch 127/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7230.3140 - mse: 7230.3140 - val_loss: 680.2692 - val_mse: 680.2692\n",
      "Epoch 128/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7230.2510 - mse: 7230.2510 - val_loss: 677.7399 - val_mse: 677.7399\n",
      "Epoch 129/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7221.7256 - mse: 7221.7256 - val_loss: 657.7316 - val_mse: 657.7316\n",
      "Epoch 130/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7232.3491 - mse: 7232.3491 - val_loss: 672.3145 - val_mse: 672.3145\n",
      "Epoch 131/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7221.2695 - mse: 7221.2695 - val_loss: 689.3365 - val_mse: 689.3365\n",
      "Epoch 132/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7230.2236 - mse: 7230.2236 - val_loss: 676.0343 - val_mse: 676.0343\n",
      "Epoch 133/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7227.1567 - mse: 7227.1567 - val_loss: 669.7700 - val_mse: 669.7700\n",
      "Epoch 134/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7225.9580 - mse: 7225.9580 - val_loss: 680.2465 - val_mse: 680.2465\n",
      "Epoch 135/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7229.2983 - mse: 7229.2983 - val_loss: 676.0129 - val_mse: 676.0129\n",
      "Epoch 136/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7226.4121 - mse: 7226.4121 - val_loss: 669.8659 - val_mse: 669.8659\n",
      "Epoch 137/150\n",
      "111/111 [==============================] - 1s 7ms/step - loss: 7222.6030 - mse: 7222.6030 - val_loss: 637.6827 - val_mse: 637.6827\n",
      "Epoch 138/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7221.3540 - mse: 7221.3540 - val_loss: 631.4576 - val_mse: 631.4576\n",
      "Epoch 139/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7229.4775 - mse: 7229.4775 - val_loss: 671.5076 - val_mse: 671.5076\n",
      "Epoch 140/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7215.3433 - mse: 7215.3433 - val_loss: 703.8672 - val_mse: 703.8672\n",
      "Epoch 141/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7217.9346 - mse: 7217.9346 - val_loss: 641.0425 - val_mse: 641.0425\n",
      "Epoch 142/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7226.4116 - mse: 7226.4116 - val_loss: 667.7363 - val_mse: 667.7363\n",
      "Epoch 143/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7227.1509 - mse: 7227.1509 - val_loss: 678.6259 - val_mse: 678.6259\n",
      "Epoch 144/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7219.2310 - mse: 7219.2310 - val_loss: 637.5790 - val_mse: 637.5790\n",
      "Epoch 145/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7228.4116 - mse: 7228.4116 - val_loss: 670.6740 - val_mse: 670.6740\n",
      "Epoch 146/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7222.8286 - mse: 7222.8286 - val_loss: 696.8853 - val_mse: 696.8853\n",
      "Epoch 147/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7224.8311 - mse: 7224.8311 - val_loss: 657.2523 - val_mse: 657.2523\n",
      "Epoch 148/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7225.7891 - mse: 7225.7891 - val_loss: 676.3441 - val_mse: 676.3441\n",
      "Epoch 149/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7222.4937 - mse: 7222.4937 - val_loss: 688.0036 - val_mse: 688.0036\n",
      "Epoch 150/150\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 7224.2456 - mse: 7224.2456 - val_loss: 673.6451 - val_mse: 673.6451\n"
     ]
    }
   ],
   "source": [
    "histories=[]\n",
    "history = fis.fit(train_features, train_label,\n",
    "                    epochs=param.n_epochs,\n",
    "                    batch_size=param.batch_size,\n",
    "                    validation_data=(test_features, test_label)\n",
    "                    \n",
    "                    )\n",
    "histories.append(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
