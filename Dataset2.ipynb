{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel(\"data2(allmax).xlsx\")\n",
    "data.to_csv('data2(allmax).csv',index=True)\n",
    "data=data.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature(max)</th>\n",
       "      <th>D.O. (mg/L)(min)</th>\n",
       "      <th>pH(max)</th>\n",
       "      <th>Conductivity(max)</th>\n",
       "      <th>BOD(max)</th>\n",
       "      <th>Total coliform(max)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>8.4</td>\n",
       "      <td>282.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>7.8</td>\n",
       "      <td>310.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>320.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>7.5</td>\n",
       "      <td>217.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>6300.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Temperature(max)  D.O. (mg/L)(min)  pH(max)  Conductivity(max)  BOD(max)  \\\n",
       "0              17.0               8.6      8.4              282.0       2.7   \n",
       "1              18.0               8.8      7.8              310.0       2.3   \n",
       "2              22.0               9.0      7.5              320.0       2.4   \n",
       "3              15.0               9.2      7.5              217.0       1.3   \n",
       "4              16.0              10.1      7.0              239.0       1.5   \n",
       "\n",
       "   Total coliform(max)  \n",
       "0               3000.0  \n",
       "1               2400.0  \n",
       "2               5000.0  \n",
       "3               3000.0  \n",
       "4               6300.0  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data,test_size=0.1,random_state=100)\n",
    "train_features = train.loc[:,data.columns !='D.O. (mg/L)(min)']\n",
    "test_features = test.loc[:,data.columns !='D.O. (mg/L)(min)']\n",
    "train_label = train['D.O. (mg/L)(min)']\n",
    "test_label = test['D.O. (mg/L)(min)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature(max)</th>\n",
       "      <th>pH(max)</th>\n",
       "      <th>Conductivity(max)</th>\n",
       "      <th>BOD(max)</th>\n",
       "      <th>Total coliform(max)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>18.0</td>\n",
       "      <td>7.30</td>\n",
       "      <td>252.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>70000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>20.0</td>\n",
       "      <td>7.40</td>\n",
       "      <td>230.0</td>\n",
       "      <td>4.50</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13.0</td>\n",
       "      <td>7.70</td>\n",
       "      <td>209.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>3900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>17.0</td>\n",
       "      <td>7.70</td>\n",
       "      <td>279.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8.40</td>\n",
       "      <td>282.0</td>\n",
       "      <td>2.70</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>20.0</td>\n",
       "      <td>7.80</td>\n",
       "      <td>175.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>30000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>16.0</td>\n",
       "      <td>6.78</td>\n",
       "      <td>231.0</td>\n",
       "      <td>1.05</td>\n",
       "      <td>13000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15.0</td>\n",
       "      <td>7.30</td>\n",
       "      <td>206.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>3400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>250.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>20.0</td>\n",
       "      <td>7.30</td>\n",
       "      <td>217.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>11000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.0</td>\n",
       "      <td>7.60</td>\n",
       "      <td>234.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>14000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>21.0</td>\n",
       "      <td>7.40</td>\n",
       "      <td>208.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>110000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>19.0</td>\n",
       "      <td>7.50</td>\n",
       "      <td>189.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>23000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>20.0</td>\n",
       "      <td>7.30</td>\n",
       "      <td>202.0</td>\n",
       "      <td>1.30</td>\n",
       "      <td>50000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>18.0</td>\n",
       "      <td>6.70</td>\n",
       "      <td>210.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>6300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>20.0</td>\n",
       "      <td>7.50</td>\n",
       "      <td>212.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>115000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>16.5</td>\n",
       "      <td>7.30</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>50000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>19.0</td>\n",
       "      <td>7.86</td>\n",
       "      <td>202.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>14000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.0</td>\n",
       "      <td>6.60</td>\n",
       "      <td>218.0</td>\n",
       "      <td>1.70</td>\n",
       "      <td>7000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>7.80</td>\n",
       "      <td>284.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>9000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>19.0</td>\n",
       "      <td>6.10</td>\n",
       "      <td>302.0</td>\n",
       "      <td>1.30</td>\n",
       "      <td>13000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>21.0</td>\n",
       "      <td>7.40</td>\n",
       "      <td>206.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>30000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>24.0</td>\n",
       "      <td>6.90</td>\n",
       "      <td>253.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>11000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>17.0</td>\n",
       "      <td>7.87</td>\n",
       "      <td>222.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15.0</td>\n",
       "      <td>7.40</td>\n",
       "      <td>222.0</td>\n",
       "      <td>1.30</td>\n",
       "      <td>11000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16.0</td>\n",
       "      <td>7.60</td>\n",
       "      <td>245.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17.0</td>\n",
       "      <td>7.70</td>\n",
       "      <td>242.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.0</td>\n",
       "      <td>7.80</td>\n",
       "      <td>310.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>19.0</td>\n",
       "      <td>7.60</td>\n",
       "      <td>204.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>90000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>239.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>6300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>16.0</td>\n",
       "      <td>7.50</td>\n",
       "      <td>222.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>15.0</td>\n",
       "      <td>7.20</td>\n",
       "      <td>249.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>11000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16.0</td>\n",
       "      <td>7.50</td>\n",
       "      <td>184.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>16.0</td>\n",
       "      <td>7.50</td>\n",
       "      <td>193.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16.0</td>\n",
       "      <td>7.60</td>\n",
       "      <td>256.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>9000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18.0</td>\n",
       "      <td>7.40</td>\n",
       "      <td>187.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>18.0</td>\n",
       "      <td>6.90</td>\n",
       "      <td>219.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>26000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>19.0</td>\n",
       "      <td>7.60</td>\n",
       "      <td>257.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>11000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>21.0</td>\n",
       "      <td>7.20</td>\n",
       "      <td>301.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>11000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>22.0</td>\n",
       "      <td>7.60</td>\n",
       "      <td>198.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>26000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.0</td>\n",
       "      <td>7.60</td>\n",
       "      <td>237.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>18.0</td>\n",
       "      <td>5.80</td>\n",
       "      <td>247.0</td>\n",
       "      <td>1.90</td>\n",
       "      <td>22000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.0</td>\n",
       "      <td>7.50</td>\n",
       "      <td>320.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>17.0</td>\n",
       "      <td>7.40</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>14000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>19.0</td>\n",
       "      <td>7.50</td>\n",
       "      <td>213.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>13000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16.0</td>\n",
       "      <td>7.40</td>\n",
       "      <td>236.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>22.0</td>\n",
       "      <td>7.50</td>\n",
       "      <td>275.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>30000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16.0</td>\n",
       "      <td>7.60</td>\n",
       "      <td>254.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>16.0</td>\n",
       "      <td>7.60</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>22.0</td>\n",
       "      <td>7.10</td>\n",
       "      <td>201.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>110000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.0</td>\n",
       "      <td>7.50</td>\n",
       "      <td>217.0</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18.0</td>\n",
       "      <td>7.40</td>\n",
       "      <td>230.0</td>\n",
       "      <td>1.90</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16.0</td>\n",
       "      <td>7.60</td>\n",
       "      <td>271.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>9000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Temperature(max)  pH(max)  Conductivity(max)  BOD(max)  \\\n",
       "56              18.0     7.30              252.0      1.20   \n",
       "32              20.0     7.40              230.0      4.50   \n",
       "6               13.0     7.70              209.0      1.20   \n",
       "23              17.0     7.70              279.0      2.40   \n",
       "0               17.0     8.40              282.0      2.70   \n",
       "43              20.0     7.80              175.0      1.80   \n",
       "65              16.0     6.78              231.0      1.05   \n",
       "12              15.0     7.30              206.0      2.40   \n",
       "5               17.0     7.00              250.0      1.40   \n",
       "61              20.0     7.30              217.0      1.00   \n",
       "20              21.0     7.60              234.0      1.50   \n",
       "45              21.0     7.40              208.0      0.80   \n",
       "42              19.0     7.50              189.0      1.20   \n",
       "46              20.0     7.30              202.0      1.30   \n",
       "28              18.0     6.70              210.0      1.50   \n",
       "52              20.0     7.50              212.0      1.20   \n",
       "37              16.5     7.30              174.0      0.70   \n",
       "48              19.0     7.86              202.0      1.85   \n",
       "19              20.0     6.60              218.0      1.70   \n",
       "8                9.0     7.80              284.0      2.50   \n",
       "63              19.0     6.10              302.0      1.30   \n",
       "47              21.0     7.40              206.0      1.10   \n",
       "62              24.0     6.90              253.0      1.40   \n",
       "39              17.0     7.87              222.0      1.00   \n",
       "21              15.0     7.40              222.0      1.30   \n",
       "24              16.0     7.60              245.0      2.40   \n",
       "15              17.0     7.70              242.0      2.30   \n",
       "1               18.0     7.80              310.0      2.30   \n",
       "35              19.0     7.60              204.0      1.00   \n",
       "4               16.0     7.00              239.0      1.50   \n",
       "30              16.0     7.50              222.0      1.10   \n",
       "59              15.0     7.20              249.0      0.60   \n",
       "25              16.0     7.50              184.0      2.10   \n",
       "33              16.0     7.50              193.0      1.00   \n",
       "10              16.0     7.60              256.0      2.60   \n",
       "18              18.0     7.40              187.0      2.00   \n",
       "41              18.0     6.90              219.0      2.30   \n",
       "60              19.0     7.60              257.0      0.60   \n",
       "64              21.0     7.20              301.0      1.40   \n",
       "53              22.0     7.60              198.0      1.50   \n",
       "16              16.0     7.60              237.0      2.10   \n",
       "55              18.0     5.80              247.0      1.90   \n",
       "2               22.0     7.50              320.0      2.40   \n",
       "38              17.0     7.40              198.0      0.80   \n",
       "34              19.0     7.50              213.0      0.40   \n",
       "11              16.0     7.40              236.0      2.20   \n",
       "54              22.0     7.50              275.0      1.20   \n",
       "17              16.0     7.60              254.0      2.20   \n",
       "26              16.0     7.60               98.0      2.30   \n",
       "44              22.0     7.10              201.0      1.50   \n",
       "3               15.0     7.50              217.0      1.30   \n",
       "27              18.0     7.40              230.0      1.90   \n",
       "9               16.0     7.60              271.0      2.40   \n",
       "\n",
       "    Total coliform(max)  \n",
       "56              70000.0  \n",
       "32                400.0  \n",
       "6                3900.0  \n",
       "23               5000.0  \n",
       "0                3000.0  \n",
       "43              30000.0  \n",
       "65              13000.0  \n",
       "12               3400.0  \n",
       "5                2300.0  \n",
       "61              11000.0  \n",
       "20              14000.0  \n",
       "45             110000.0  \n",
       "42              23000.0  \n",
       "46              50000.0  \n",
       "28               6300.0  \n",
       "52             115000.0  \n",
       "37              50000.0  \n",
       "48              14000.0  \n",
       "19               7000.0  \n",
       "8                9000.0  \n",
       "63              13000.0  \n",
       "47              30000.0  \n",
       "62              11000.0  \n",
       "39               1300.0  \n",
       "21              11000.0  \n",
       "24               3000.0  \n",
       "15               2200.0  \n",
       "1                2400.0  \n",
       "35              90000.0  \n",
       "4                6300.0  \n",
       "30               2200.0  \n",
       "59              11000.0  \n",
       "25               3000.0  \n",
       "33               3000.0  \n",
       "10               9000.0  \n",
       "18               3600.0  \n",
       "41              26000.0  \n",
       "60              11000.0  \n",
       "64              11000.0  \n",
       "53              26000.0  \n",
       "16               2400.0  \n",
       "55              22000.0  \n",
       "2                5000.0  \n",
       "38              14000.0  \n",
       "34              13000.0  \n",
       "11               3000.0  \n",
       "54              30000.0  \n",
       "17               1700.0  \n",
       "26               2200.0  \n",
       "44             110000.0  \n",
       "3                3000.0  \n",
       "27               4000.0  \n",
       "9                9000.0  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53, 5)\n",
      "(53,)\n",
      "(6, 5)\n",
      "(6,)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape)\n",
    "print(train_label.shape)\n",
    "print(test_features.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler=preprocessing.StandardScaler()\n",
    "# # scaler=preprocessing.MaxAbsScaler()\n",
    "# train_features=scaler.fit_transform(train_features)\n",
    "# test_features=scaler.fit_transform(test_features)\n",
    "train_features = preprocessing.MinMaxScaler(feature_range=(0, 1)).fit_transform(train_features)\n",
    "test_features = preprocessing.MinMaxScaler(feature_range=(0, 1)).fit_transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANFIS MODEL CODE\n",
    "from Models import myanfis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9223372036854775807"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.maxsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = myanfis.fis_parameters(\n",
    "    n_input=5,                # no. of Regressors\n",
    "    n_memb=6,                 # no. of fuzzy memberships\n",
    "    batch_size=1,            # 16 / 32 / 64 / ...\n",
    "    memb_func='gaussian',      # 'gaussian' / 'gbellmf'\n",
    "    optimizer='sgd',          # sgd / adam / ...\n",
    "    # mse / mae / huber_loss / mean_absolute_percentage_error / ...\n",
    "    loss='mse',\n",
    "    n_epochs=150               # 10 / 25 / 50 / 100 / ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "fis = myanfis.ANFIS(n_input=param.n_input,\n",
    "                    n_memb=param.n_memb,\n",
    "                    batch_size=param.batch_size,\n",
    "                    memb_func=param.memb_func,\n",
    "                    name='myanfis'\n",
    "                    )\n",
    "\n",
    "# fis = myanfis.ANFIS(n_input=6,\n",
    "#                     n_memb=3,\n",
    "#                     batch_size=1,\n",
    "#                     memb_func='gaussian',\n",
    "#                     name='myanfis'\n",
    "#                     )\n",
    "# compile model\n",
    "fis.model.compile(optimizer=param.optimizer,\n",
    "                    loss=param.loss,\n",
    "                    metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "53/53 [==============================] - 1s 7ms/step - loss: 86.0141 - mse: 86.0141 - val_loss: 72.1334 - val_mse: 72.1334\n",
      "Epoch 2/150\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 60.6012 - mse: 60.6012 - val_loss: 25.7159 - val_mse: 25.7159\n",
      "Epoch 3/150\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8.2014 - mse: 8.2014 - val_loss: 4.5258 - val_mse: 4.5258\n",
      "Epoch 4/150\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 2.3665 - mse: 2.3665 - val_loss: 4.6299 - val_mse: 4.6299\n",
      "Epoch 5/150\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 2.0299 - mse: 2.0299 - val_loss: 2.5094 - val_mse: 2.5094\n",
      "Epoch 6/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.8367 - mse: 1.8367 - val_loss: 2.1873 - val_mse: 2.1873\n",
      "Epoch 7/150\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.0305 - mse: 2.0305 - val_loss: 1.7310 - val_mse: 1.7310\n",
      "Epoch 8/150\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.4653 - mse: 1.4653 - val_loss: 3.8677 - val_mse: 3.8677\n",
      "Epoch 9/150\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.9257 - mse: 1.9257 - val_loss: 1.9535 - val_mse: 1.9535\n",
      "Epoch 10/150\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.7314 - mse: 1.7314 - val_loss: 1.2302 - val_mse: 1.2302\n",
      "Epoch 11/150\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.7759 - mse: 1.7759 - val_loss: 1.0183 - val_mse: 1.0183\n",
      "Epoch 12/150\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 1.7381 - mse: 1.7381 - val_loss: 0.9272 - val_mse: 0.9272\n",
      "Epoch 13/150\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 1.6452 - mse: 1.6452 - val_loss: 1.4308 - val_mse: 1.4308\n",
      "Epoch 14/150\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.6431 - mse: 1.6431 - val_loss: 1.1142 - val_mse: 1.1142\n",
      "Epoch 15/150\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.7218 - mse: 1.7218 - val_loss: 0.8967 - val_mse: 0.8967\n",
      "Epoch 16/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.5466 - mse: 1.5466 - val_loss: 1.0846 - val_mse: 1.0846\n",
      "Epoch 17/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.6635 - mse: 1.6635 - val_loss: 0.9742 - val_mse: 0.9742\n",
      "Epoch 18/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.4460 - mse: 1.4460 - val_loss: 0.7021 - val_mse: 0.7021\n",
      "Epoch 19/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.6032 - mse: 1.6032 - val_loss: 0.5711 - val_mse: 0.5711\n",
      "Epoch 20/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.6161 - mse: 1.6161 - val_loss: 1.0518 - val_mse: 1.0518\n",
      "Epoch 21/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.5155 - mse: 1.5155 - val_loss: 1.7217 - val_mse: 1.7217\n",
      "Epoch 22/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.6295 - mse: 1.6295 - val_loss: 0.9356 - val_mse: 0.9356\n",
      "Epoch 23/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.4080 - mse: 1.4080 - val_loss: 0.5798 - val_mse: 0.5798\n",
      "Epoch 24/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.4002 - mse: 1.4002 - val_loss: 0.5681 - val_mse: 0.5681\n",
      "Epoch 25/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.5340 - mse: 1.5340 - val_loss: 0.6948 - val_mse: 0.6948\n",
      "Epoch 26/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.5346 - mse: 1.5346 - val_loss: 0.7900 - val_mse: 0.7900\n",
      "Epoch 27/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.4480 - mse: 1.4480 - val_loss: 0.7955 - val_mse: 0.7955\n",
      "Epoch 28/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.5235 - mse: 1.5235 - val_loss: 1.0741 - val_mse: 1.0741\n",
      "Epoch 29/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.4384 - mse: 1.4384 - val_loss: 0.6139 - val_mse: 0.6139\n",
      "Epoch 30/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.5317 - mse: 1.5317 - val_loss: 0.6314 - val_mse: 0.6314\n",
      "Epoch 31/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.3616 - mse: 1.3616 - val_loss: 1.2296 - val_mse: 1.2296\n",
      "Epoch 32/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.4853 - mse: 1.4853 - val_loss: 0.7413 - val_mse: 0.7413\n",
      "Epoch 33/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.5087 - mse: 1.5087 - val_loss: 0.9295 - val_mse: 0.9295\n",
      "Epoch 34/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.4359 - mse: 1.4359 - val_loss: 0.7173 - val_mse: 0.7173\n",
      "Epoch 35/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.3224 - mse: 1.3224 - val_loss: 0.6537 - val_mse: 0.6537\n",
      "Epoch 36/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.5897 - mse: 1.5897 - val_loss: 0.5548 - val_mse: 0.5548\n",
      "Epoch 37/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.3682 - mse: 1.3682 - val_loss: 0.8077 - val_mse: 0.8077\n",
      "Epoch 38/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.4692 - mse: 1.4692 - val_loss: 0.5179 - val_mse: 0.5179\n",
      "Epoch 39/150\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.4260 - mse: 1.4260 - val_loss: 0.6020 - val_mse: 0.6020\n",
      "Epoch 40/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.3287 - mse: 1.3287 - val_loss: 1.3127 - val_mse: 1.3127\n",
      "Epoch 41/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.4304 - mse: 1.4304 - val_loss: 0.5254 - val_mse: 0.5254\n",
      "Epoch 42/150\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.3033 - mse: 1.3033 - val_loss: 1.2131 - val_mse: 1.2131\n",
      "Epoch 43/150\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.4085 - mse: 1.4085 - val_loss: 0.5137 - val_mse: 0.5137\n",
      "Epoch 44/150\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.2304 - mse: 1.2304 - val_loss: 0.8710 - val_mse: 0.8710\n",
      "Epoch 45/150\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.3584 - mse: 1.3584 - val_loss: 1.0538 - val_mse: 1.0538\n",
      "Epoch 46/150\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 1.3829 - mse: 1.3829 - val_loss: 0.9371 - val_mse: 0.9371\n",
      "Epoch 47/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.4306 - mse: 1.4306 - val_loss: 0.5767 - val_mse: 0.5767\n",
      "Epoch 48/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.3967 - mse: 1.3967 - val_loss: 0.4868 - val_mse: 0.4868\n",
      "Epoch 49/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.3995 - mse: 1.3995 - val_loss: 0.6904 - val_mse: 0.6904\n",
      "Epoch 50/150\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.3477 - mse: 1.3477 - val_loss: 0.7589 - val_mse: 0.7589\n",
      "Epoch 51/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.3547 - mse: 1.3547 - val_loss: 0.3719 - val_mse: 0.3719\n",
      "Epoch 52/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.3378 - mse: 1.3378 - val_loss: 0.6451 - val_mse: 0.6451\n",
      "Epoch 53/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.3464 - mse: 1.3464 - val_loss: 0.4595 - val_mse: 0.4595\n",
      "Epoch 54/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.4025 - mse: 1.4025 - val_loss: 0.6017 - val_mse: 0.6017\n",
      "Epoch 55/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.3029 - mse: 1.3029 - val_loss: 0.5260 - val_mse: 0.5260\n",
      "Epoch 56/150\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.2637 - mse: 1.2637 - val_loss: 1.0008 - val_mse: 1.0008\n",
      "Epoch 57/150\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.2662 - mse: 1.2662 - val_loss: 0.6858 - val_mse: 0.6858\n",
      "Epoch 58/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2326 - mse: 1.2326 - val_loss: 0.9616 - val_mse: 0.9616\n",
      "Epoch 59/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.3548 - mse: 1.3548 - val_loss: 0.5731 - val_mse: 0.5731\n",
      "Epoch 60/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2455 - mse: 1.2455 - val_loss: 0.7470 - val_mse: 0.7470\n",
      "Epoch 61/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.4382 - mse: 1.4382 - val_loss: 0.4740 - val_mse: 0.4740\n",
      "Epoch 62/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.3480 - mse: 1.3480 - val_loss: 0.4753 - val_mse: 0.4753\n",
      "Epoch 63/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2811 - mse: 1.2811 - val_loss: 1.0370 - val_mse: 1.0370\n",
      "Epoch 64/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2745 - mse: 1.2745 - val_loss: 0.5659 - val_mse: 0.5659\n",
      "Epoch 65/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.3099 - mse: 1.3099 - val_loss: 0.6052 - val_mse: 0.6052\n",
      "Epoch 66/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2628 - mse: 1.2628 - val_loss: 0.9821 - val_mse: 0.9821\n",
      "Epoch 67/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.3633 - mse: 1.3633 - val_loss: 0.4726 - val_mse: 0.4726\n",
      "Epoch 68/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2650 - mse: 1.2650 - val_loss: 0.4628 - val_mse: 0.4628\n",
      "Epoch 69/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2713 - mse: 1.2713 - val_loss: 0.5166 - val_mse: 0.5166\n",
      "Epoch 70/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2961 - mse: 1.2961 - val_loss: 0.6340 - val_mse: 0.6340\n",
      "Epoch 71/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2805 - mse: 1.2805 - val_loss: 0.3443 - val_mse: 0.3443\n",
      "Epoch 72/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2624 - mse: 1.2624 - val_loss: 1.1475 - val_mse: 1.1475\n",
      "Epoch 73/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2798 - mse: 1.2798 - val_loss: 0.6796 - val_mse: 0.6796\n",
      "Epoch 74/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1923 - mse: 1.1923 - val_loss: 1.0540 - val_mse: 1.0540\n",
      "Epoch 75/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.3474 - mse: 1.3474 - val_loss: 0.4295 - val_mse: 0.4295\n",
      "Epoch 76/150\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.1257 - mse: 1.1257 - val_loss: 1.4673 - val_mse: 1.4673\n",
      "Epoch 77/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2859 - mse: 1.2859 - val_loss: 0.6567 - val_mse: 0.6567\n",
      "Epoch 78/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2592 - mse: 1.2592 - val_loss: 0.5534 - val_mse: 0.5534\n",
      "Epoch 79/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2205 - mse: 1.2205 - val_loss: 0.5570 - val_mse: 0.5570\n",
      "Epoch 80/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.0374 - mse: 1.0374 - val_loss: 0.3302 - val_mse: 0.3302\n",
      "Epoch 81/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1992 - mse: 1.1992 - val_loss: 0.6531 - val_mse: 0.6531\n",
      "Epoch 82/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.3093 - mse: 1.3093 - val_loss: 0.7528 - val_mse: 0.7528\n",
      "Epoch 83/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2598 - mse: 1.2598 - val_loss: 0.6257 - val_mse: 0.6257\n",
      "Epoch 84/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2258 - mse: 1.2258 - val_loss: 1.2562 - val_mse: 1.2562\n",
      "Epoch 85/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2128 - mse: 1.2128 - val_loss: 0.6739 - val_mse: 0.6739\n",
      "Epoch 86/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2108 - mse: 1.2108 - val_loss: 1.1738 - val_mse: 1.1738\n",
      "Epoch 87/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1525 - mse: 1.1525 - val_loss: 0.6267 - val_mse: 0.6267\n",
      "Epoch 88/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2101 - mse: 1.2101 - val_loss: 0.6357 - val_mse: 0.6357\n",
      "Epoch 89/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2769 - mse: 1.2769 - val_loss: 0.5848 - val_mse: 0.5848\n",
      "Epoch 90/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1848 - mse: 1.1848 - val_loss: 0.6987 - val_mse: 0.6987\n",
      "Epoch 91/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1604 - mse: 1.1604 - val_loss: 1.4142 - val_mse: 1.4142\n",
      "Epoch 92/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2777 - mse: 1.2777 - val_loss: 0.5663 - val_mse: 0.5663\n",
      "Epoch 93/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2084 - mse: 1.2084 - val_loss: 0.6204 - val_mse: 0.6204\n",
      "Epoch 94/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1260 - mse: 1.1260 - val_loss: 0.9071 - val_mse: 0.9071\n",
      "Epoch 95/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1995 - mse: 1.1995 - val_loss: 0.8232 - val_mse: 0.8232\n",
      "Epoch 96/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2418 - mse: 1.2418 - val_loss: 0.6554 - val_mse: 0.6554\n",
      "Epoch 97/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2373 - mse: 1.2373 - val_loss: 0.8471 - val_mse: 0.8471\n",
      "Epoch 98/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2279 - mse: 1.2279 - val_loss: 0.4947 - val_mse: 0.4947\n",
      "Epoch 99/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1660 - mse: 1.1660 - val_loss: 0.4474 - val_mse: 0.4474\n",
      "Epoch 100/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2303 - mse: 1.2303 - val_loss: 0.5127 - val_mse: 0.5127\n",
      "Epoch 101/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2750 - mse: 1.2750 - val_loss: 0.5174 - val_mse: 0.5174\n",
      "Epoch 102/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2209 - mse: 1.2209 - val_loss: 0.5162 - val_mse: 0.5162\n",
      "Epoch 103/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.0736 - mse: 1.0736 - val_loss: 0.6454 - val_mse: 0.6454\n",
      "Epoch 104/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1747 - mse: 1.1747 - val_loss: 0.8261 - val_mse: 0.8261\n",
      "Epoch 105/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1770 - mse: 1.1770 - val_loss: 0.7643 - val_mse: 0.7643\n",
      "Epoch 106/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1174 - mse: 1.1174 - val_loss: 0.4042 - val_mse: 0.4042\n",
      "Epoch 107/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1856 - mse: 1.1856 - val_loss: 0.5413 - val_mse: 0.5413\n",
      "Epoch 108/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2446 - mse: 1.2446 - val_loss: 0.3586 - val_mse: 0.3586\n",
      "Epoch 109/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.0920 - mse: 1.0920 - val_loss: 0.5560 - val_mse: 0.5560\n",
      "Epoch 110/150\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 1.1125 - mse: 1.1125 - val_loss: 0.4185 - val_mse: 0.4185\n",
      "Epoch 111/150\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 1.1266 - mse: 1.1266 - val_loss: 0.5676 - val_mse: 0.5676\n",
      "Epoch 112/150\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.0513 - mse: 1.0513 - val_loss: 1.2271 - val_mse: 1.2271\n",
      "Epoch 113/150\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 1.1284 - mse: 1.1284 - val_loss: 0.4870 - val_mse: 0.4870\n",
      "Epoch 114/150\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.0040 - mse: 1.0040 - val_loss: 0.5989 - val_mse: 0.5989\n",
      "Epoch 115/150\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.1822 - mse: 1.1822 - val_loss: 0.4374 - val_mse: 0.4374\n",
      "Epoch 116/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.0984 - mse: 1.0984 - val_loss: 0.7307 - val_mse: 0.7307\n",
      "Epoch 117/150\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 1.1788 - mse: 1.1788 - val_loss: 1.1791 - val_mse: 1.1791\n",
      "Epoch 118/150\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 1.0780 - mse: 1.0780 - val_loss: 1.0024 - val_mse: 1.0024\n",
      "Epoch 119/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1582 - mse: 1.1582 - val_loss: 0.5294 - val_mse: 0.5294\n",
      "Epoch 120/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1846 - mse: 1.1846 - val_loss: 0.5983 - val_mse: 0.5983\n",
      "Epoch 121/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1661 - mse: 1.1661 - val_loss: 1.0030 - val_mse: 1.0030\n",
      "Epoch 122/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.0918 - mse: 1.0918 - val_loss: 0.4810 - val_mse: 0.4810\n",
      "Epoch 123/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1713 - mse: 1.1713 - val_loss: 0.6447 - val_mse: 0.6447\n",
      "Epoch 124/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1531 - mse: 1.1531 - val_loss: 0.7273 - val_mse: 0.7273\n",
      "Epoch 125/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1806 - mse: 1.1806 - val_loss: 0.8091 - val_mse: 0.8091\n",
      "Epoch 126/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.0847 - mse: 1.0847 - val_loss: 0.9633 - val_mse: 0.9633\n",
      "Epoch 127/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1496 - mse: 1.1496 - val_loss: 0.7418 - val_mse: 0.7418\n",
      "Epoch 128/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1370 - mse: 1.1370 - val_loss: 1.2164 - val_mse: 1.2164\n",
      "Epoch 129/150\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.1649 - mse: 1.1649 - val_loss: 0.4696 - val_mse: 0.4696\n",
      "Epoch 130/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1195 - mse: 1.1195 - val_loss: 1.1245 - val_mse: 1.1245\n",
      "Epoch 131/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1667 - mse: 1.1667 - val_loss: 0.8634 - val_mse: 0.8634\n",
      "Epoch 132/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1767 - mse: 1.1767 - val_loss: 1.1126 - val_mse: 1.1126\n",
      "Epoch 133/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1446 - mse: 1.1446 - val_loss: 0.7617 - val_mse: 0.7617\n",
      "Epoch 134/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.2207 - mse: 1.2207 - val_loss: 0.5415 - val_mse: 0.5415\n",
      "Epoch 135/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1319 - mse: 1.1319 - val_loss: 1.0528 - val_mse: 1.0528\n",
      "Epoch 136/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.0092 - mse: 1.0092 - val_loss: 0.8271 - val_mse: 0.8271\n",
      "Epoch 137/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1482 - mse: 1.1482 - val_loss: 0.5088 - val_mse: 0.5088\n",
      "Epoch 138/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1136 - mse: 1.1136 - val_loss: 0.8514 - val_mse: 0.8514\n",
      "Epoch 139/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1443 - mse: 1.1443 - val_loss: 0.6477 - val_mse: 0.6477\n",
      "Epoch 140/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1814 - mse: 1.1814 - val_loss: 0.7769 - val_mse: 0.7769\n",
      "Epoch 141/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1272 - mse: 1.1272 - val_loss: 0.9569 - val_mse: 0.9569\n",
      "Epoch 142/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1169 - mse: 1.1169 - val_loss: 0.5856 - val_mse: 0.5856\n",
      "Epoch 143/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1712 - mse: 1.1712 - val_loss: 0.6815 - val_mse: 0.6815\n",
      "Epoch 144/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1413 - mse: 1.1413 - val_loss: 0.7199 - val_mse: 0.7199\n",
      "Epoch 145/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1718 - mse: 1.1718 - val_loss: 0.6165 - val_mse: 0.6165\n",
      "Epoch 146/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.0115 - mse: 1.0115 - val_loss: 1.0386 - val_mse: 1.0386\n",
      "Epoch 147/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1072 - mse: 1.1072 - val_loss: 0.9650 - val_mse: 0.9650\n",
      "Epoch 148/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1802 - mse: 1.1802 - val_loss: 0.8546 - val_mse: 0.8546\n",
      "Epoch 149/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.1296 - mse: 1.1296 - val_loss: 0.4830 - val_mse: 0.4830\n",
      "Epoch 150/150\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 1.0980 - mse: 1.0980 - val_loss: 1.0651 - val_mse: 1.0651\n"
     ]
    }
   ],
   "source": [
    "histories=[]\n",
    "history = fis.fit(train_features, train_label,\n",
    "                    epochs=param.n_epochs,\n",
    "                    batch_size=param.batch_size,\n",
    "                    validation_data=(test_features, test_label)\n",
    "                    \n",
    "                    )\n",
    "histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step\n",
      "[[9.448664]\n",
      " [9.395324]\n",
      " [9.441611]\n",
      " [8.078151]\n",
      " [9.581496]\n",
      " [8.092948]]\n"
     ]
    }
   ],
   "source": [
    "test_pred=fis(test_features)\n",
    "\n",
    "print(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual =  8.6 predicted =  9.45\n",
      "actual =  8.5 predicted =  9.4\n",
      "actual =  9.9 predicted =  9.44\n",
      "actual =  8.5 predicted =  8.08\n",
      "actual =  8.2 predicted =  9.58\n",
      "actual =  7.6 predicted =  8.09\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(\"MAE:\", round(mean_absolute_error(test_label, test_pred),2))\n",
    "# print(\"MAPE = \", round(mean_absolute_percentage_error(test_label, test_pred),2))\n",
    "# print(\"MSE = \", round(mean_squared_error(test_label, test_pred),2))\n",
    "# print(\"R2_score = \", round(r2_score(test_label, test_pred),2))\n",
    "for i,j in zip(test_label,test_pred):\n",
    "    print(\"actual = \",round(i,2), \"predicted = \",round(j[0],2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
